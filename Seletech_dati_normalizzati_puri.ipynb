{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seletech-dati-normalizzati-puri.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtegegn/deepdream/blob/master/Seletech_dati_normalizzati_puri.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKMejpdTMs0f",
        "colab_type": "text"
      },
      "source": [
        "<h1> Seletech classificazione dati normalizzati di materiali puri</h1>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACJUKr2uNJoe",
        "colab_type": "text"
      },
      "source": [
        "# Import Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSu4bYQH25NV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2\n",
        "!pip install tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZmwPTfE1vxb",
        "colab_type": "code",
        "outputId": "ae64853f-d65c-4abe-fe93-a549e918d745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jfSXSAbNRcS",
        "colab_type": "text"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhi5SzwRNByy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "from keras import optimizers \n",
        "#from keras.models import Sequential \n",
        "#from keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras import Sequential\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from sklearn.pipeline import Pipeline \n",
        "import matplotlib.pylab as plt \n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "import tensorflow as tf \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck3-67XIecli",
        "colab_type": "text"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIH0k9xSNMiR",
        "colab_type": "code",
        "outputId": "0f41110d-fbe4-4183-97c5-c017359b36af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load dataset\n",
        "dataframe = pd.read_csv(\"/content/drive/My Drive/Seletech/table/tablePureNormMaterial.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "X = dataset[:,0:702].astype(float)\n",
        "Y = dataset[:,702]\n",
        "print(dataset.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7200, 703)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc_WfxOKehQC",
        "colab_type": "text"
      },
      "source": [
        "## Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGpHAjwcQEsV",
        "colab_type": "code",
        "outputId": "7da7e1f2-e4f4-4c2d-b789-b6a86e87c807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(np.unique(Y))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2)\n",
        "\n",
        "print(\"\\nX_train:\\n\")\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(\"\\nX_test:\\n\")\n",
        "print(X_test.shape)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7200, 702)\n",
            "(7200,)\n",
            "['BabyMilk' 'Cocoa' 'CornFlour' 'CornStarch' 'PotatoStarch' 'RiceStarch'\n",
            " 'Sugar' 'WheatStarch']\n",
            "\n",
            "X_train:\n",
            "\n",
            "(5760, 702)\n",
            "(5760,)\n",
            "\n",
            "X_test:\n",
            "\n",
            "(1440, 702)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVR4Ve4VTZnt",
        "colab_type": "code",
        "outputId": "fa2eecaa-3bb8-4ba5-ad32-eb8ecfda6244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y_train)\n",
        "encoded_Y = encoder.transform(y_train)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)\n",
        "\n",
        "print(dummy_y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5760, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4xU5ubE7iHw",
        "colab_type": "code",
        "outputId": "7105af86-14e5-4b75-bf59-f78330e98758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y_test)\n",
        "encoded_Y = encoder.transform(y_test)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y_test = np_utils.to_categorical(encoded_Y)\n",
        "\n",
        "print(dummy_y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1440, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCyc1JAFgdHe",
        "colab_type": "text"
      },
      "source": [
        "# Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXg9K6YagjJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
        "\t# create model\n",
        "\tgrid_model = Sequential()\n",
        "\tgrid_model.add(Dense(8, input_dim=702, kernel_initializer=init, activation='relu'))\n",
        "\tgrid_model.add(Dense(256, kernel_initializer=init, activation='relu'))\n",
        "\tgrid_model.add(Dense(8, kernel_initializer=init, activation='softmax'))\n",
        "\t# Compile model\n",
        "\tgrid_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
        "\treturn grid_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5l3VJjEgcZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "10bee350-ee0f-47c8-ffd2-eac381cf206a"
      },
      "source": [
        " # fix random seed for reproducibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# create model\n",
        "grid_model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "# grid search epochs, batch size and optimizer\n",
        "optimizers = ['rmsprop', 'adam']\n",
        "init = ['glorot_uniform', 'normal', 'uniform']\n",
        "epochs = [50, 100, 150]\n",
        "batches = [5, 10, 20]\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=grid_model, param_grid=param_grid, n_jobs=-1, cv=5)\n",
        "grid_result = grid.fit(X_train,y_train)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.953646 using {'batch_size': 5, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.118403 (0.001299) with: {'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.891319 (0.044845) with: {'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.773264 (0.097702) with: {'batch_size': 5, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.644097 (0.370742) with: {'batch_size': 5, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.391840 (0.386947) with: {'batch_size': 5, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.392014 (0.387192) with: {'batch_size': 5, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.332639 (0.303959) with: {'batch_size': 5, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.639931 (0.370293) with: {'batch_size': 5, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.344444 (0.320654) with: {'batch_size': 5, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.922743 (0.035743) with: {'batch_size': 5, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.390278 (0.383262) with: {'batch_size': 5, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.677431 (0.395074) with: {'batch_size': 5, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.382118 (0.373933) with: {'batch_size': 5, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.384028 (0.374423) with: {'batch_size': 5, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.626562 (0.363500) with: {'batch_size': 5, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.953646 (0.015934) with: {'batch_size': 5, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.669792 (0.391211) with: {'batch_size': 5, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.910243 (0.021827) with: {'batch_size': 5, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.323958 (0.289472) with: {'batch_size': 10, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.590799 (0.347418) with: {'batch_size': 10, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.535590 (0.294757) with: {'batch_size': 10, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.822222 (0.058809) with: {'batch_size': 10, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.802951 (0.090687) with: {'batch_size': 10, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.907813 (0.068189) with: {'batch_size': 10, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.463368 (0.280043) with: {'batch_size': 10, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.896007 (0.028315) with: {'batch_size': 10, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.878472 (0.076283) with: {'batch_size': 10, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.865972 (0.089207) with: {'batch_size': 10, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.913368 (0.040759) with: {'batch_size': 10, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.668229 (0.390023) with: {'batch_size': 10, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.751736 (0.125855) with: {'batch_size': 10, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.397569 (0.395049) with: {'batch_size': 10, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.664236 (0.386073) with: {'batch_size': 10, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.928472 (0.025149) with: {'batch_size': 10, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.118403 (0.001299) with: {'batch_size': 10, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.667361 (0.387283) with: {'batch_size': 10, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.355729 (0.336613) with: {'batch_size': 20, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.646875 (0.373310) with: {'batch_size': 20, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.885069 (0.009756) with: {'batch_size': 20, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.640104 (0.367917) with: {'batch_size': 20, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.290104 (0.241595) with: {'batch_size': 20, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.914410 (0.011173) with: {'batch_size': 20, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.667535 (0.387318) with: {'batch_size': 20, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.924479 (0.024210) with: {'batch_size': 20, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.737847 (0.138393) with: {'batch_size': 20, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.673438 (0.392482) with: {'batch_size': 20, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.609028 (0.351934) with: {'batch_size': 20, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.933681 (0.018703) with: {'batch_size': 20, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.389236 (0.383999) with: {'batch_size': 20, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.939236 (0.015341) with: {'batch_size': 20, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.610069 (0.347075) with: {'batch_size': 20, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.932118 (0.022997) with: {'batch_size': 20, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.626910 (0.362856) with: {'batch_size': 20, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.945660 (0.015382) with: {'batch_size': 20, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4y04cM-EZYF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "7127184d-fdd8-4d55-bae9-e9a2ff4ebf99"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(\"Detailed classification report:\")\n",
        "print()\n",
        "print(\"The model is trained on the full development set.\")\n",
        "print(\"The scores are computed on the full evaluation set.\")\n",
        "print()\n",
        "y_true, y_pred = y_test, grid.predict(X_test)\n",
        "print(classification_report(y_true, y_pred))\n",
        "print()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detailed classification report:\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    BabyMilk       0.97      1.00      0.98       179\n",
            "       Cocoa       1.00      1.00      1.00       182\n",
            "   CornFlour       1.00      0.97      0.98       186\n",
            "  CornStarch       0.87      0.80      0.83       171\n",
            "PotatoStarch       1.00      1.00      1.00       176\n",
            "  RiceStarch       0.98      1.00      0.99       194\n",
            "       Sugar       1.00      1.00      1.00       174\n",
            " WheatStarch       0.83      0.89      0.86       178\n",
            "\n",
            "    accuracy                           0.96      1440\n",
            "   macro avg       0.96      0.96      0.96      1440\n",
            "weighted avg       0.96      0.96      0.96      1440\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orT73hZ3iSq5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "49a7ae9a-7de8-4f0e-d010-73210c006843"
      },
      "source": [
        "\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(y_pred,y_true))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "[[179   0   6   0   0   0   0   0]\n",
            " [  0 182   0   0   0   0   0   0]\n",
            " [  0   0 180   0   0   0   0   0]\n",
            " [  0   0   0 136   0   0   0  20]\n",
            " [  0   0   0   0 176   0   0   0]\n",
            " [  0   0   0   3   0 194   0   0]\n",
            " [  0   0   0   0   0   0 174   0]\n",
            " [  0   0   0  32   0   0   0 158]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTqFhxXpenIY",
        "colab_type": "text"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzG-9tMXHkBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline_model():\n",
        "  #create model\n",
        "  model= Sequential()\n",
        "  model.add(Dense(8, input_dim=702, activation = 'relu'))\n",
        "  model.add(Dense(8, activation='softmax'))\n",
        "  #compile model\n",
        "  with tf.device('/GPU:0'):\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkvpRjdwesZg",
        "colab_type": "text"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-jyTui8quVx",
        "colab_type": "code",
        "outputId": "3f1fac8b-8b25-4e5f-a84f-c8f02a898e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Compile the model\n",
        "model = baseline_model()\n",
        "model.summary()\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "               optimizer=optimizers.RMSprop(lr=1e-4), \n",
        "               metrics=['acc'])\n",
        "\n",
        "# Train the Model\n",
        "with tf.device('/GPU:0'):\n",
        "  history = model.fit(x=X_train, \n",
        "                      y=dummy_y, \n",
        "                      validation_data = (X_test,dummy_y_test),\n",
        "                      batch_size=30, \n",
        "                      epochs=200, \n",
        "                      verbose=1\n",
        "                      )\n",
        "\n",
        "# Save the Model\n",
        "#model.save('/content/drive/My Drive/artigianale.h5')\n",
        "\n",
        "# Plot the accuracy and loss curves\n",
        "acc = history.history['acc']\n",
        "#val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "#val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "#plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "#plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 8)                 5624      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 72        \n",
            "=================================================================\n",
            "Total params: 5,696\n",
            "Trainable params: 5,696\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 5760 samples, validate on 1440 samples\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "5760/5760 [==============================] - 5s 898us/step - loss: 2.0682 - acc: 0.1503 - val_loss: 2.0266 - val_acc: 0.2208\n",
            "Epoch 2/200\n",
            "5760/5760 [==============================] - 1s 110us/step - loss: 2.0035 - acc: 0.2194 - val_loss: 1.9718 - val_acc: 0.2271\n",
            "Epoch 3/200\n",
            "5760/5760 [==============================] - 1s 96us/step - loss: 1.9520 - acc: 0.2377 - val_loss: 1.9212 - val_acc: 0.2465\n",
            "Epoch 4/200\n",
            "5760/5760 [==============================] - 1s 121us/step - loss: 1.9009 - acc: 0.2441 - val_loss: 1.8751 - val_acc: 0.2368\n",
            "Epoch 5/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 1.8544 - acc: 0.2436 - val_loss: 1.8281 - val_acc: 0.2479\n",
            "Epoch 6/200\n",
            "5760/5760 [==============================] - 1s 96us/step - loss: 1.8095 - acc: 0.2439 - val_loss: 1.7885 - val_acc: 0.2444\n",
            "Epoch 7/200\n",
            "5760/5760 [==============================] - 1s 97us/step - loss: 1.7665 - acc: 0.2443 - val_loss: 1.7459 - val_acc: 0.2514\n",
            "Epoch 8/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 1.7265 - acc: 0.2415 - val_loss: 1.7023 - val_acc: 0.2417\n",
            "Epoch 9/200\n",
            "5760/5760 [==============================] - 1s 109us/step - loss: 1.6862 - acc: 0.2354 - val_loss: 1.6641 - val_acc: 0.2285\n",
            "Epoch 10/200\n",
            "5760/5760 [==============================] - 1s 92us/step - loss: 1.6464 - acc: 0.2026 - val_loss: 1.6243 - val_acc: 0.2139\n",
            "Epoch 11/200\n",
            "5760/5760 [==============================] - 1s 92us/step - loss: 1.6083 - acc: 0.2361 - val_loss: 1.5855 - val_acc: 0.2431\n",
            "Epoch 12/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 1.5705 - acc: 0.2502 - val_loss: 1.5472 - val_acc: 0.2681\n",
            "Epoch 13/200\n",
            "5760/5760 [==============================] - 1s 107us/step - loss: 1.5332 - acc: 0.2720 - val_loss: 1.5103 - val_acc: 0.2993\n",
            "Epoch 14/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 1.4975 - acc: 0.3170 - val_loss: 1.4741 - val_acc: 0.3618\n",
            "Epoch 15/200\n",
            "5760/5760 [==============================] - 1s 94us/step - loss: 1.4627 - acc: 0.3464 - val_loss: 1.4428 - val_acc: 0.3528\n",
            "Epoch 16/200\n",
            "5760/5760 [==============================] - 1s 101us/step - loss: 1.4297 - acc: 0.3340 - val_loss: 1.4105 - val_acc: 0.3542\n",
            "Epoch 17/200\n",
            "5760/5760 [==============================] - 1s 91us/step - loss: 1.3992 - acc: 0.3260 - val_loss: 1.3779 - val_acc: 0.3201\n",
            "Epoch 18/200\n",
            "5760/5760 [==============================] - 1s 98us/step - loss: 1.3697 - acc: 0.3255 - val_loss: 1.3507 - val_acc: 0.3403\n",
            "Epoch 19/200\n",
            "5760/5760 [==============================] - 1s 103us/step - loss: 1.3419 - acc: 0.3345 - val_loss: 1.3223 - val_acc: 0.3382\n",
            "Epoch 20/200\n",
            "5760/5760 [==============================] - 1s 94us/step - loss: 1.3145 - acc: 0.3375 - val_loss: 1.2980 - val_acc: 0.3528\n",
            "Epoch 21/200\n",
            "5760/5760 [==============================] - 1s 91us/step - loss: 1.2889 - acc: 0.3451 - val_loss: 1.2697 - val_acc: 0.3611\n",
            "Epoch 22/200\n",
            "5760/5760 [==============================] - 1s 107us/step - loss: 1.2629 - acc: 0.3559 - val_loss: 1.2458 - val_acc: 0.3826\n",
            "Epoch 23/200\n",
            "5760/5760 [==============================] - 1s 113us/step - loss: 1.2374 - acc: 0.4040 - val_loss: 1.2204 - val_acc: 0.4097\n",
            "Epoch 24/200\n",
            "5760/5760 [==============================] - 1s 110us/step - loss: 1.2126 - acc: 0.4083 - val_loss: 1.1959 - val_acc: 0.4146\n",
            "Epoch 25/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 1.1892 - acc: 0.3991 - val_loss: 1.1741 - val_acc: 0.4243\n",
            "Epoch 26/200\n",
            "5760/5760 [==============================] - 1s 108us/step - loss: 1.1664 - acc: 0.3970 - val_loss: 1.1491 - val_acc: 0.3771\n",
            "Epoch 27/200\n",
            "5760/5760 [==============================] - 1s 94us/step - loss: 1.1426 - acc: 0.4132 - val_loss: 1.1267 - val_acc: 0.4188\n",
            "Epoch 28/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 1.1201 - acc: 0.4792 - val_loss: 1.1049 - val_acc: 0.4931\n",
            "Epoch 29/200\n",
            "5760/5760 [==============================] - 1s 89us/step - loss: 1.0978 - acc: 0.5439 - val_loss: 1.0822 - val_acc: 0.5479\n",
            "Epoch 30/200\n",
            "5760/5760 [==============================] - 1s 98us/step - loss: 1.0751 - acc: 0.5592 - val_loss: 1.0622 - val_acc: 0.5681\n",
            "Epoch 31/200\n",
            "5760/5760 [==============================] - 1s 110us/step - loss: 1.0531 - acc: 0.5764 - val_loss: 1.0412 - val_acc: 0.5722\n",
            "Epoch 32/200\n",
            "5760/5760 [==============================] - 1s 101us/step - loss: 1.0309 - acc: 0.5941 - val_loss: 1.0191 - val_acc: 0.5910\n",
            "Epoch 33/200\n",
            "5760/5760 [==============================] - 1s 102us/step - loss: 1.0095 - acc: 0.6163 - val_loss: 0.9960 - val_acc: 0.6146\n",
            "Epoch 34/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 0.9883 - acc: 0.6441 - val_loss: 0.9752 - val_acc: 0.6382\n",
            "Epoch 35/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 0.9671 - acc: 0.6720 - val_loss: 0.9558 - val_acc: 0.6618\n",
            "Epoch 36/200\n",
            "5760/5760 [==============================] - 1s 89us/step - loss: 0.9471 - acc: 0.7146 - val_loss: 0.9355 - val_acc: 0.7236\n",
            "Epoch 37/200\n",
            "5760/5760 [==============================] - 1s 91us/step - loss: 0.9280 - acc: 0.7530 - val_loss: 0.9206 - val_acc: 0.6979\n",
            "Epoch 38/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 0.9082 - acc: 0.7816 - val_loss: 0.8977 - val_acc: 0.7625\n",
            "Epoch 39/200\n",
            "5760/5760 [==============================] - 1s 96us/step - loss: 0.8884 - acc: 0.7917 - val_loss: 0.8789 - val_acc: 0.7958\n",
            "Epoch 40/200\n",
            "5760/5760 [==============================] - 1s 124us/step - loss: 0.8701 - acc: 0.8033 - val_loss: 0.8616 - val_acc: 0.7979\n",
            "Epoch 41/200\n",
            "5760/5760 [==============================] - 1s 106us/step - loss: 0.8521 - acc: 0.8158 - val_loss: 0.8416 - val_acc: 0.8062\n",
            "Epoch 42/200\n",
            "5760/5760 [==============================] - 1s 92us/step - loss: 0.8333 - acc: 0.8194 - val_loss: 0.8238 - val_acc: 0.8118\n",
            "Epoch 43/200\n",
            "5760/5760 [==============================] - 1s 96us/step - loss: 0.8154 - acc: 0.8255 - val_loss: 0.8060 - val_acc: 0.8139\n",
            "Epoch 44/200\n",
            "5760/5760 [==============================] - 1s 102us/step - loss: 0.7978 - acc: 0.8326 - val_loss: 0.7907 - val_acc: 0.8062\n",
            "Epoch 45/200\n",
            "5760/5760 [==============================] - 1s 88us/step - loss: 0.7809 - acc: 0.8292 - val_loss: 0.7731 - val_acc: 0.8167\n",
            "Epoch 46/200\n",
            "5760/5760 [==============================] - 1s 103us/step - loss: 0.7643 - acc: 0.8328 - val_loss: 0.7586 - val_acc: 0.8222\n",
            "Epoch 47/200\n",
            "5760/5760 [==============================] - 1s 123us/step - loss: 0.7476 - acc: 0.8411 - val_loss: 0.7400 - val_acc: 0.8174\n",
            "Epoch 48/200\n",
            "5760/5760 [==============================] - 1s 95us/step - loss: 0.7316 - acc: 0.8424 - val_loss: 0.7262 - val_acc: 0.8312\n",
            "Epoch 49/200\n",
            "5760/5760 [==============================] - 1s 92us/step - loss: 0.7166 - acc: 0.8392 - val_loss: 0.7103 - val_acc: 0.8278\n",
            "Epoch 50/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 0.7010 - acc: 0.8382 - val_loss: 0.6957 - val_acc: 0.8285\n",
            "Epoch 51/200\n",
            "5760/5760 [==============================] - 1s 92us/step - loss: 0.6861 - acc: 0.8446 - val_loss: 0.6827 - val_acc: 0.8403\n",
            "Epoch 52/200\n",
            "5760/5760 [==============================] - 1s 101us/step - loss: 0.6720 - acc: 0.8519 - val_loss: 0.6677 - val_acc: 0.8479\n",
            "Epoch 53/200\n",
            "5760/5760 [==============================] - 1s 87us/step - loss: 0.6570 - acc: 0.8616 - val_loss: 0.6519 - val_acc: 0.8403\n",
            "Epoch 54/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 0.6427 - acc: 0.8604 - val_loss: 0.6392 - val_acc: 0.8437\n",
            "Epoch 55/200\n",
            "5760/5760 [==============================] - 1s 88us/step - loss: 0.6289 - acc: 0.8526 - val_loss: 0.6253 - val_acc: 0.8340\n",
            "Epoch 56/200\n",
            "5760/5760 [==============================] - 1s 112us/step - loss: 0.6152 - acc: 0.8512 - val_loss: 0.6121 - val_acc: 0.8424\n",
            "Epoch 57/200\n",
            "5760/5760 [==============================] - 1s 121us/step - loss: 0.6019 - acc: 0.8622 - val_loss: 0.6002 - val_acc: 0.8431\n",
            "Epoch 58/200\n",
            "5760/5760 [==============================] - 1s 100us/step - loss: 0.5897 - acc: 0.8564 - val_loss: 0.5871 - val_acc: 0.8472\n",
            "Epoch 59/200\n",
            "5760/5760 [==============================] - 1s 98us/step - loss: 0.5773 - acc: 0.8635 - val_loss: 0.5755 - val_acc: 0.8479\n",
            "Epoch 60/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 0.5652 - acc: 0.8554 - val_loss: 0.5642 - val_acc: 0.8556\n",
            "Epoch 61/200\n",
            "5760/5760 [==============================] - 1s 94us/step - loss: 0.5536 - acc: 0.8703 - val_loss: 0.5543 - val_acc: 0.8583\n",
            "Epoch 62/200\n",
            "5760/5760 [==============================] - 1s 87us/step - loss: 0.5421 - acc: 0.8655 - val_loss: 0.5426 - val_acc: 0.8646\n",
            "Epoch 63/200\n",
            "5760/5760 [==============================] - 1s 100us/step - loss: 0.5311 - acc: 0.8677 - val_loss: 0.5325 - val_acc: 0.8528\n",
            "Epoch 64/200\n",
            "5760/5760 [==============================] - 1s 116us/step - loss: 0.5203 - acc: 0.8726 - val_loss: 0.5210 - val_acc: 0.8604\n",
            "Epoch 65/200\n",
            "5760/5760 [==============================] - 1s 99us/step - loss: 0.5098 - acc: 0.8790 - val_loss: 0.5112 - val_acc: 0.8653\n",
            "Epoch 66/200\n",
            "5760/5760 [==============================] - 1s 95us/step - loss: 0.4999 - acc: 0.8698 - val_loss: 0.5015 - val_acc: 0.8646\n",
            "Epoch 67/200\n",
            "5760/5760 [==============================] - 1s 98us/step - loss: 0.4898 - acc: 0.8786 - val_loss: 0.4920 - val_acc: 0.8653\n",
            "Epoch 68/200\n",
            "5760/5760 [==============================] - 1s 96us/step - loss: 0.4807 - acc: 0.8774 - val_loss: 0.4857 - val_acc: 0.8708\n",
            "Epoch 69/200\n",
            "5760/5760 [==============================] - 1s 94us/step - loss: 0.4717 - acc: 0.8734 - val_loss: 0.4779 - val_acc: 0.8812\n",
            "Epoch 70/200\n",
            "5760/5760 [==============================] - 1s 98us/step - loss: 0.4630 - acc: 0.8830 - val_loss: 0.4672 - val_acc: 0.8639\n",
            "Epoch 71/200\n",
            "5760/5760 [==============================] - 1s 92us/step - loss: 0.4549 - acc: 0.8835 - val_loss: 0.4598 - val_acc: 0.8715\n",
            "Epoch 72/200\n",
            "5760/5760 [==============================] - 1s 92us/step - loss: 0.4468 - acc: 0.8762 - val_loss: 0.4521 - val_acc: 0.8799\n",
            "Epoch 73/200\n",
            "5760/5760 [==============================] - 1s 103us/step - loss: 0.4394 - acc: 0.8788 - val_loss: 0.4438 - val_acc: 0.8792\n",
            "Epoch 74/200\n",
            "5760/5760 [==============================] - 1s 106us/step - loss: 0.4317 - acc: 0.8977 - val_loss: 0.4374 - val_acc: 0.8729\n",
            "Epoch 75/200\n",
            "5760/5760 [==============================] - 1s 121us/step - loss: 0.4242 - acc: 0.8826 - val_loss: 0.4301 - val_acc: 0.8743\n",
            "Epoch 76/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 0.4175 - acc: 0.8840 - val_loss: 0.4226 - val_acc: 0.8757\n",
            "Epoch 77/200\n",
            "5760/5760 [==============================] - 1s 107us/step - loss: 0.4107 - acc: 0.8825 - val_loss: 0.4189 - val_acc: 0.8819\n",
            "Epoch 78/200\n",
            "5760/5760 [==============================] - 1s 108us/step - loss: 0.4043 - acc: 0.8852 - val_loss: 0.4120 - val_acc: 0.8840\n",
            "Epoch 79/200\n",
            "5760/5760 [==============================] - 1s 95us/step - loss: 0.3982 - acc: 0.8908 - val_loss: 0.4041 - val_acc: 0.8764\n",
            "Epoch 80/200\n",
            "5760/5760 [==============================] - 1s 96us/step - loss: 0.3919 - acc: 0.8918 - val_loss: 0.3980 - val_acc: 0.8785\n",
            "Epoch 81/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 0.3867 - acc: 0.8854 - val_loss: 0.3937 - val_acc: 0.8931\n",
            "Epoch 82/200\n",
            "5760/5760 [==============================] - 1s 120us/step - loss: 0.3809 - acc: 0.8997 - val_loss: 0.3882 - val_acc: 0.8910\n",
            "Epoch 83/200\n",
            "5760/5760 [==============================] - 1s 92us/step - loss: 0.3757 - acc: 0.9047 - val_loss: 0.3866 - val_acc: 0.9028\n",
            "Epoch 84/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 0.3701 - acc: 0.8941 - val_loss: 0.3770 - val_acc: 0.8896\n",
            "Epoch 85/200\n",
            "5760/5760 [==============================] - 1s 107us/step - loss: 0.3651 - acc: 0.8995 - val_loss: 0.3723 - val_acc: 0.8924\n",
            "Epoch 86/200\n",
            "5760/5760 [==============================] - 1s 102us/step - loss: 0.3604 - acc: 0.9036 - val_loss: 0.3679 - val_acc: 0.8944\n",
            "Epoch 87/200\n",
            "5760/5760 [==============================] - 1s 94us/step - loss: 0.3558 - acc: 0.9003 - val_loss: 0.3628 - val_acc: 0.9104\n",
            "Epoch 88/200\n",
            "5760/5760 [==============================] - 1s 89us/step - loss: 0.3513 - acc: 0.9066 - val_loss: 0.3634 - val_acc: 0.8944\n",
            "Epoch 89/200\n",
            "5760/5760 [==============================] - 1s 95us/step - loss: 0.3465 - acc: 0.9122 - val_loss: 0.3563 - val_acc: 0.8917\n",
            "Epoch 90/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 0.3422 - acc: 0.9118 - val_loss: 0.3511 - val_acc: 0.8986\n",
            "Epoch 91/200\n",
            "5760/5760 [==============================] - 1s 94us/step - loss: 0.3381 - acc: 0.9080 - val_loss: 0.3478 - val_acc: 0.9042\n",
            "Epoch 92/200\n",
            "5760/5760 [==============================] - 1s 89us/step - loss: 0.3338 - acc: 0.9123 - val_loss: 0.3420 - val_acc: 0.9187\n",
            "Epoch 93/200\n",
            "5760/5760 [==============================] - 1s 95us/step - loss: 0.3297 - acc: 0.9132 - val_loss: 0.3383 - val_acc: 0.9125\n",
            "Epoch 94/200\n",
            "5760/5760 [==============================] - 1s 88us/step - loss: 0.3257 - acc: 0.9142 - val_loss: 0.3357 - val_acc: 0.9000\n",
            "Epoch 95/200\n",
            "5760/5760 [==============================] - 1s 94us/step - loss: 0.3217 - acc: 0.9134 - val_loss: 0.3320 - val_acc: 0.9153\n",
            "Epoch 96/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 0.3182 - acc: 0.9099 - val_loss: 0.3279 - val_acc: 0.8972\n",
            "Epoch 97/200\n",
            "5760/5760 [==============================] - 1s 118us/step - loss: 0.3146 - acc: 0.9135 - val_loss: 0.3228 - val_acc: 0.9111\n",
            "Epoch 98/200\n",
            "5760/5760 [==============================] - 1s 101us/step - loss: 0.3112 - acc: 0.9111 - val_loss: 0.3200 - val_acc: 0.9056\n",
            "Epoch 99/200\n",
            "5760/5760 [==============================] - 1s 116us/step - loss: 0.3074 - acc: 0.9111 - val_loss: 0.3163 - val_acc: 0.9111\n",
            "Epoch 100/200\n",
            "5760/5760 [==============================] - 1s 99us/step - loss: 0.3044 - acc: 0.9089 - val_loss: 0.3149 - val_acc: 0.8993\n",
            "Epoch 101/200\n",
            "5760/5760 [==============================] - 1s 95us/step - loss: 0.3011 - acc: 0.9092 - val_loss: 0.3095 - val_acc: 0.9111\n",
            "Epoch 102/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 0.2980 - acc: 0.9078 - val_loss: 0.3076 - val_acc: 0.9000\n",
            "Epoch 103/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 0.2950 - acc: 0.9080 - val_loss: 0.3043 - val_acc: 0.9042\n",
            "Epoch 104/200\n",
            "5760/5760 [==============================] - 1s 105us/step - loss: 0.2917 - acc: 0.9078 - val_loss: 0.3025 - val_acc: 0.8986\n",
            "Epoch 105/200\n",
            "5760/5760 [==============================] - 1s 101us/step - loss: 0.2881 - acc: 0.9090 - val_loss: 0.3053 - val_acc: 0.8882\n",
            "Epoch 106/200\n",
            "5760/5760 [==============================] - 1s 116us/step - loss: 0.2858 - acc: 0.9073 - val_loss: 0.2949 - val_acc: 0.9111\n",
            "Epoch 107/200\n",
            "5760/5760 [==============================] - 1s 119us/step - loss: 0.2828 - acc: 0.9085 - val_loss: 0.2959 - val_acc: 0.8910\n",
            "Epoch 108/200\n",
            "5760/5760 [==============================] - 1s 117us/step - loss: 0.2800 - acc: 0.9083 - val_loss: 0.2910 - val_acc: 0.9132\n",
            "Epoch 109/200\n",
            "5760/5760 [==============================] - 1s 113us/step - loss: 0.2773 - acc: 0.9090 - val_loss: 0.2877 - val_acc: 0.9111\n",
            "Epoch 110/200\n",
            "5760/5760 [==============================] - 1s 89us/step - loss: 0.2750 - acc: 0.9069 - val_loss: 0.2841 - val_acc: 0.9069\n",
            "Epoch 111/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 0.2723 - acc: 0.9083 - val_loss: 0.2836 - val_acc: 0.8958\n",
            "Epoch 112/200\n",
            "5760/5760 [==============================] - 1s 88us/step - loss: 0.2701 - acc: 0.9075 - val_loss: 0.2804 - val_acc: 0.9014\n",
            "Epoch 113/200\n",
            "5760/5760 [==============================] - 1s 118us/step - loss: 0.2673 - acc: 0.9083 - val_loss: 0.2768 - val_acc: 0.9069\n",
            "Epoch 114/200\n",
            "5760/5760 [==============================] - 1s 107us/step - loss: 0.2652 - acc: 0.9073 - val_loss: 0.2760 - val_acc: 0.9090\n",
            "Epoch 115/200\n",
            "5760/5760 [==============================] - 1s 92us/step - loss: 0.2626 - acc: 0.9095 - val_loss: 0.2751 - val_acc: 0.9007\n",
            "Epoch 116/200\n",
            "5760/5760 [==============================] - 1s 99us/step - loss: 0.2605 - acc: 0.9089 - val_loss: 0.2717 - val_acc: 0.9021\n",
            "Epoch 117/200\n",
            "5760/5760 [==============================] - 1s 114us/step - loss: 0.2584 - acc: 0.9097 - val_loss: 0.2679 - val_acc: 0.9111\n",
            "Epoch 118/200\n",
            "5760/5760 [==============================] - 1s 102us/step - loss: 0.2563 - acc: 0.9092 - val_loss: 0.2658 - val_acc: 0.9118\n",
            "Epoch 119/200\n",
            "5760/5760 [==============================] - 1s 95us/step - loss: 0.2540 - acc: 0.9120 - val_loss: 0.2697 - val_acc: 0.9014\n",
            "Epoch 120/200\n",
            "5760/5760 [==============================] - 1s 107us/step - loss: 0.2522 - acc: 0.9080 - val_loss: 0.2618 - val_acc: 0.9111\n",
            "Epoch 121/200\n",
            "5760/5760 [==============================] - 1s 94us/step - loss: 0.2505 - acc: 0.9102 - val_loss: 0.2594 - val_acc: 0.9111\n",
            "Epoch 122/200\n",
            "5760/5760 [==============================] - 1s 108us/step - loss: 0.2481 - acc: 0.9104 - val_loss: 0.2582 - val_acc: 0.9083\n",
            "Epoch 123/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 0.2463 - acc: 0.9102 - val_loss: 0.2566 - val_acc: 0.9111\n",
            "Epoch 124/200\n",
            "5760/5760 [==============================] - 1s 88us/step - loss: 0.2448 - acc: 0.9094 - val_loss: 0.2541 - val_acc: 0.9097\n",
            "Epoch 125/200\n",
            "5760/5760 [==============================] - 1s 98us/step - loss: 0.2428 - acc: 0.9123 - val_loss: 0.2546 - val_acc: 0.9076\n",
            "Epoch 126/200\n",
            "5760/5760 [==============================] - 1s 88us/step - loss: 0.2404 - acc: 0.9125 - val_loss: 0.2503 - val_acc: 0.9125\n",
            "Epoch 127/200\n",
            "5760/5760 [==============================] - 1s 96us/step - loss: 0.2389 - acc: 0.9144 - val_loss: 0.2519 - val_acc: 0.9056\n",
            "Epoch 128/200\n",
            "5760/5760 [==============================] - 1s 99us/step - loss: 0.2374 - acc: 0.9109 - val_loss: 0.2484 - val_acc: 0.9035\n",
            "Epoch 129/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 0.2356 - acc: 0.9099 - val_loss: 0.2472 - val_acc: 0.9083\n",
            "Epoch 130/200\n",
            "5760/5760 [==============================] - 1s 91us/step - loss: 0.2340 - acc: 0.9116 - val_loss: 0.2452 - val_acc: 0.9104\n",
            "Epoch 131/200\n",
            "5760/5760 [==============================] - 1s 99us/step - loss: 0.2324 - acc: 0.9127 - val_loss: 0.2417 - val_acc: 0.9111\n",
            "Epoch 132/200\n",
            "5760/5760 [==============================] - 1s 112us/step - loss: 0.2308 - acc: 0.9116 - val_loss: 0.2408 - val_acc: 0.9139\n",
            "Epoch 133/200\n",
            "5760/5760 [==============================] - 1s 92us/step - loss: 0.2293 - acc: 0.9139 - val_loss: 0.2397 - val_acc: 0.9125\n",
            "Epoch 134/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 0.2281 - acc: 0.9120 - val_loss: 0.2387 - val_acc: 0.9118\n",
            "Epoch 135/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 0.2263 - acc: 0.9153 - val_loss: 0.2378 - val_acc: 0.9021\n",
            "Epoch 136/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 0.2253 - acc: 0.9118 - val_loss: 0.2348 - val_acc: 0.9083\n",
            "Epoch 137/200\n",
            "5760/5760 [==============================] - 1s 103us/step - loss: 0.2233 - acc: 0.9118 - val_loss: 0.2332 - val_acc: 0.9125\n",
            "Epoch 138/200\n",
            "5760/5760 [==============================] - 1s 88us/step - loss: 0.2218 - acc: 0.9123 - val_loss: 0.2322 - val_acc: 0.9146\n",
            "Epoch 139/200\n",
            "5760/5760 [==============================] - 1s 104us/step - loss: 0.2204 - acc: 0.9167 - val_loss: 0.2314 - val_acc: 0.9111\n",
            "Epoch 140/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 0.2196 - acc: 0.9153 - val_loss: 0.2293 - val_acc: 0.9104\n",
            "Epoch 141/200\n",
            "5760/5760 [==============================] - 1s 113us/step - loss: 0.2181 - acc: 0.9122 - val_loss: 0.2273 - val_acc: 0.9118\n",
            "Epoch 142/200\n",
            "5760/5760 [==============================] - 1s 102us/step - loss: 0.2171 - acc: 0.9139 - val_loss: 0.2262 - val_acc: 0.9118\n",
            "Epoch 143/200\n",
            "5760/5760 [==============================] - 1s 96us/step - loss: 0.2150 - acc: 0.9156 - val_loss: 0.2254 - val_acc: 0.9146\n",
            "Epoch 144/200\n",
            "5760/5760 [==============================] - 1s 100us/step - loss: 0.2141 - acc: 0.9167 - val_loss: 0.2235 - val_acc: 0.9118\n",
            "Epoch 145/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 0.2127 - acc: 0.9151 - val_loss: 0.2230 - val_acc: 0.9097\n",
            "Epoch 146/200\n",
            "5760/5760 [==============================] - 1s 95us/step - loss: 0.2117 - acc: 0.9137 - val_loss: 0.2214 - val_acc: 0.9090\n",
            "Epoch 147/200\n",
            "5760/5760 [==============================] - 1s 114us/step - loss: 0.2109 - acc: 0.9156 - val_loss: 0.2208 - val_acc: 0.9104\n",
            "Epoch 148/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 0.2091 - acc: 0.9174 - val_loss: 0.2201 - val_acc: 0.9139\n",
            "Epoch 149/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 0.2083 - acc: 0.9151 - val_loss: 0.2198 - val_acc: 0.9049\n",
            "Epoch 150/200\n",
            "5760/5760 [==============================] - 1s 91us/step - loss: 0.2076 - acc: 0.9142 - val_loss: 0.2171 - val_acc: 0.9118\n",
            "Epoch 151/200\n",
            "5760/5760 [==============================] - 1s 89us/step - loss: 0.2063 - acc: 0.9161 - val_loss: 0.2157 - val_acc: 0.9118\n",
            "Epoch 152/200\n",
            "5760/5760 [==============================] - 1s 100us/step - loss: 0.2047 - acc: 0.9142 - val_loss: 0.2146 - val_acc: 0.9118\n",
            "Epoch 153/200\n",
            "5760/5760 [==============================] - 1s 94us/step - loss: 0.2039 - acc: 0.9189 - val_loss: 0.2141 - val_acc: 0.9097\n",
            "Epoch 154/200\n",
            "5760/5760 [==============================] - 1s 99us/step - loss: 0.2027 - acc: 0.9175 - val_loss: 0.2247 - val_acc: 0.8965\n",
            "Epoch 155/200\n",
            "5760/5760 [==============================] - 1s 88us/step - loss: 0.2023 - acc: 0.9181 - val_loss: 0.2115 - val_acc: 0.9118\n",
            "Epoch 156/200\n",
            "5760/5760 [==============================] - 1s 113us/step - loss: 0.2009 - acc: 0.9174 - val_loss: 0.2102 - val_acc: 0.9167\n",
            "Epoch 157/200\n",
            "5760/5760 [==============================] - 1s 111us/step - loss: 0.2003 - acc: 0.9179 - val_loss: 0.2101 - val_acc: 0.9139\n",
            "Epoch 158/200\n",
            "5760/5760 [==============================] - 1s 99us/step - loss: 0.1995 - acc: 0.9160 - val_loss: 0.2094 - val_acc: 0.9153\n",
            "Epoch 159/200\n",
            "5760/5760 [==============================] - 1s 119us/step - loss: 0.1983 - acc: 0.9179 - val_loss: 0.2085 - val_acc: 0.9083\n",
            "Epoch 160/200\n",
            "5760/5760 [==============================] - 1s 94us/step - loss: 0.1970 - acc: 0.9175 - val_loss: 0.2100 - val_acc: 0.9035\n",
            "Epoch 161/200\n",
            "5760/5760 [==============================] - 1s 116us/step - loss: 0.1968 - acc: 0.9186 - val_loss: 0.2076 - val_acc: 0.9056\n",
            "Epoch 162/200\n",
            "5760/5760 [==============================] - 1s 120us/step - loss: 0.1957 - acc: 0.9191 - val_loss: 0.2047 - val_acc: 0.9139\n",
            "Epoch 163/200\n",
            "5760/5760 [==============================] - 1s 97us/step - loss: 0.1949 - acc: 0.9160 - val_loss: 0.2060 - val_acc: 0.9174\n",
            "Epoch 164/200\n",
            "5760/5760 [==============================] - 1s 101us/step - loss: 0.1935 - acc: 0.9179 - val_loss: 0.2051 - val_acc: 0.9097\n",
            "Epoch 165/200\n",
            "5760/5760 [==============================] - 1s 89us/step - loss: 0.1930 - acc: 0.9181 - val_loss: 0.2022 - val_acc: 0.9139\n",
            "Epoch 166/200\n",
            "5760/5760 [==============================] - 1s 95us/step - loss: 0.1923 - acc: 0.9193 - val_loss: 0.2015 - val_acc: 0.9160\n",
            "Epoch 167/200\n",
            "5760/5760 [==============================] - 1s 94us/step - loss: 0.1915 - acc: 0.9181 - val_loss: 0.2014 - val_acc: 0.9125\n",
            "Epoch 168/200\n",
            "5760/5760 [==============================] - 1s 107us/step - loss: 0.1906 - acc: 0.9182 - val_loss: 0.2004 - val_acc: 0.9160\n",
            "Epoch 169/200\n",
            "5760/5760 [==============================] - 1s 115us/step - loss: 0.1899 - acc: 0.9196 - val_loss: 0.1993 - val_acc: 0.9132\n",
            "Epoch 170/200\n",
            "5760/5760 [==============================] - 1s 119us/step - loss: 0.1888 - acc: 0.9200 - val_loss: 0.1995 - val_acc: 0.9132\n",
            "Epoch 171/200\n",
            "5760/5760 [==============================] - 1s 115us/step - loss: 0.1884 - acc: 0.9198 - val_loss: 0.1982 - val_acc: 0.9187\n",
            "Epoch 172/200\n",
            "5760/5760 [==============================] - 1s 96us/step - loss: 0.1877 - acc: 0.9198 - val_loss: 0.1990 - val_acc: 0.9090\n",
            "Epoch 173/200\n",
            "5760/5760 [==============================] - 1s 105us/step - loss: 0.1871 - acc: 0.9184 - val_loss: 0.1959 - val_acc: 0.9174\n",
            "Epoch 174/200\n",
            "5760/5760 [==============================] - 1s 98us/step - loss: 0.1864 - acc: 0.9194 - val_loss: 0.1973 - val_acc: 0.9153\n",
            "Epoch 175/200\n",
            "5760/5760 [==============================] - 1s 107us/step - loss: 0.1856 - acc: 0.9189 - val_loss: 0.1949 - val_acc: 0.9194\n",
            "Epoch 176/200\n",
            "5760/5760 [==============================] - 1s 93us/step - loss: 0.1847 - acc: 0.9207 - val_loss: 0.1944 - val_acc: 0.9153\n",
            "Epoch 177/200\n",
            "5760/5760 [==============================] - 1s 101us/step - loss: 0.1844 - acc: 0.9194 - val_loss: 0.1947 - val_acc: 0.9146\n",
            "Epoch 178/200\n",
            "5760/5760 [==============================] - 1s 103us/step - loss: 0.1834 - acc: 0.9214 - val_loss: 0.1930 - val_acc: 0.9146\n",
            "Epoch 179/200\n",
            "5760/5760 [==============================] - 1s 107us/step - loss: 0.1826 - acc: 0.9217 - val_loss: 0.1918 - val_acc: 0.9160\n",
            "Epoch 180/200\n",
            "5760/5760 [==============================] - 1s 106us/step - loss: 0.1819 - acc: 0.9215 - val_loss: 0.1931 - val_acc: 0.9118\n",
            "Epoch 181/200\n",
            "5760/5760 [==============================] - 1s 91us/step - loss: 0.1815 - acc: 0.9212 - val_loss: 0.1918 - val_acc: 0.9160\n",
            "Epoch 182/200\n",
            "5760/5760 [==============================] - 1s 92us/step - loss: 0.1807 - acc: 0.9201 - val_loss: 0.1899 - val_acc: 0.9194\n",
            "Epoch 183/200\n",
            "5760/5760 [==============================] - 1s 105us/step - loss: 0.1800 - acc: 0.9212 - val_loss: 0.1891 - val_acc: 0.9201\n",
            "Epoch 184/200\n",
            "5760/5760 [==============================] - 1s 94us/step - loss: 0.1797 - acc: 0.9220 - val_loss: 0.1886 - val_acc: 0.9181\n",
            "Epoch 185/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 0.1789 - acc: 0.9227 - val_loss: 0.1884 - val_acc: 0.9201\n",
            "Epoch 186/200\n",
            "5760/5760 [==============================] - 1s 107us/step - loss: 0.1784 - acc: 0.9219 - val_loss: 0.1873 - val_acc: 0.9215\n",
            "Epoch 187/200\n",
            "5760/5760 [==============================] - 1s 121us/step - loss: 0.1772 - acc: 0.9227 - val_loss: 0.1868 - val_acc: 0.9208\n",
            "Epoch 188/200\n",
            "5760/5760 [==============================] - 1s 91us/step - loss: 0.1769 - acc: 0.9219 - val_loss: 0.1873 - val_acc: 0.9250\n",
            "Epoch 189/200\n",
            "5760/5760 [==============================] - 1s 92us/step - loss: 0.1767 - acc: 0.9220 - val_loss: 0.1857 - val_acc: 0.9167\n",
            "Epoch 190/200\n",
            "5760/5760 [==============================] - 1s 105us/step - loss: 0.1756 - acc: 0.9240 - val_loss: 0.1852 - val_acc: 0.9215\n",
            "Epoch 191/200\n",
            "5760/5760 [==============================] - 1s 95us/step - loss: 0.1753 - acc: 0.9234 - val_loss: 0.1899 - val_acc: 0.9083\n",
            "Epoch 192/200\n",
            "5760/5760 [==============================] - 1s 96us/step - loss: 0.1750 - acc: 0.9215 - val_loss: 0.1852 - val_acc: 0.9174\n",
            "Epoch 193/200\n",
            "5760/5760 [==============================] - 1s 110us/step - loss: 0.1745 - acc: 0.9212 - val_loss: 0.1846 - val_acc: 0.9229\n",
            "Epoch 194/200\n",
            "5760/5760 [==============================] - 1s 90us/step - loss: 0.1741 - acc: 0.9219 - val_loss: 0.1837 - val_acc: 0.9174\n",
            "Epoch 195/200\n",
            "5760/5760 [==============================] - 1s 111us/step - loss: 0.1734 - acc: 0.9226 - val_loss: 0.1869 - val_acc: 0.9083\n",
            "Epoch 196/200\n",
            "5760/5760 [==============================] - 1s 110us/step - loss: 0.1729 - acc: 0.9231 - val_loss: 0.1863 - val_acc: 0.9111\n",
            "Epoch 197/200\n",
            "5760/5760 [==============================] - 1s 121us/step - loss: 0.1721 - acc: 0.9250 - val_loss: 0.1824 - val_acc: 0.9250\n",
            "Epoch 198/200\n",
            "5760/5760 [==============================] - 1s 94us/step - loss: 0.1718 - acc: 0.9227 - val_loss: 0.1812 - val_acc: 0.9194\n",
            "Epoch 199/200\n",
            "5760/5760 [==============================] - 1s 103us/step - loss: 0.1716 - acc: 0.9229 - val_loss: 0.1803 - val_acc: 0.9201\n",
            "Epoch 200/200\n",
            "5760/5760 [==============================] - 1s 119us/step - loss: 0.1709 - acc: 0.9247 - val_loss: 0.1796 - val_acc: 0.9222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEQp0EZEcoUAXEckRCnQRkRzx/wGcMSVlGrp1KAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxtxB4I_ex2t",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwHcUsQfAqsL",
        "colab_type": "code",
        "outputId": "4beb796c-38db-44ff-8e4d-48c4118cd06b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "print(model.evaluate_generator(generator = X_train, verbose=1))\n",
        "print(model.evaluate_generator(generator = validation_generator, verbose=1))\n",
        "print(model.evaluate_generator(generator = test_generator, verbose=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-28468ef4924d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peTtyk1Qe4ch",
        "colab_type": "text"
      },
      "source": [
        "# Test Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7-r_Nc69VqE",
        "colab_type": "code",
        "outputId": "701a2ea9-d424-4a0e-efa4-d754c5bb106d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "y_pred = model.predict(X_test)\n",
        "predicted_classes = np.argmax(y_pred ,axis=1)\n",
        "print(y_test.shape)\n",
        "print(y_pred.shape)\n",
        "print(predicted_classes.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1440,)\n",
            "(1440, 8)\n",
            "(1440,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm_BmSTPS-yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "# Get the filenames from the generator\n",
        "\n",
        "# Get the ground truth from generator\n",
        "ground_truth = dummy_y_test\n",
        "\n",
        "\n",
        "\n",
        "# Get the predictions from the model using the generator\n",
        "\n",
        "\n",
        "errors = np.where(predicted_classes != ground_truth)[0]\n",
        "print(\"No of errors = {}/{}\".format(len(errors),len(X_test)))\n",
        "\n",
        "\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(y_pred.argmax(axis=1),predicted_classes))\n",
        "print('Classification Report')\n",
        "target_names = numpy.unique(Y)\n",
        "print(classification_report(y_pred.argmax(axis=1),predicted_classes, target_names=target_names))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E4uy8bASXOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "estimator = KerasClassifier(build_fn=baseline_model, epochs=150, batch_size=10, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ja0kYZde-yg",
        "colab_type": "text"
      },
      "source": [
        "# K-fold Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-FHaZFfSbwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold = KFold(n_splits=5, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fun65ausSfQa",
        "colab_type": "code",
        "outputId": "52fea59d-55f1-43d0-c0a7-46d54d52fcf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline: 87.97% (5.53%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHSWjDZTZDxI",
        "colab_type": "code",
        "outputId": "701859d2-3338-4469-b485-9acbb71b7716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8340277780985667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf_Xi20OO8OW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}